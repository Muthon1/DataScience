{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN92VqNzG9SCAZrIjkC9mh7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muthon1/DataScience/blob/main/phase_2_Decision_Tree_and_Naive_Bayes_Classifiers_for_Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Difference between Decision Tree and Naive Bayes Classifiers\n",
        "Naive Bayes classifier is a probabilistic classifier based on the Bayes' theorem which assumes that features are conditionally independent given the class. It works well with high-dimensional data and is therefore commonly used for text classification tasks such as spam detection, document categorization and sentiment analysis.\n",
        "\n",
        "Decision tree classifier on the other hand is a tree-based model that splits data based on feature values without any assumptions of independence. It works well for both classification and regression problems and is often used where model interpretability is important."
      ],
      "metadata": {
        "id": "zmQUGDVXxLXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the models\n",
        "\n",
        "# Install required libraries\n",
        "!pip install numpy pandas scikit-learn matplotlib\n",
        "\n",
        "# Importing the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mVUSgmiX2jBQ",
        "outputId": "fc7f2b27-084b-4735-8903-d48be7ac307b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the 20 Newsgroup dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n"
      ],
      "metadata": {
        "id": "oEuyLInm21dX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing the text data\n",
        "# Vectorize text using TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(newsgroups.data)\n",
        "y = newsgroups.target\n",
        "\n",
        "# Split data into training and test sets\n",
        "# Use the vectorized data 'X' instead of 'newsgroups.data'\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree model\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "dt_predictions = dt_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Decision Tree Classifier Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, dt_predictions)}\")\n",
        "print(classification_report(y_test, dt_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFw-GwYGEKtl",
        "outputId": "76f54281-f665-4d09-ef32-7c15186106e4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier Performance:\n",
            "Accuracy: 0.46047745358090186\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.34      0.34       151\n",
            "           1       0.39      0.36      0.37       202\n",
            "           2       0.44      0.43      0.43       195\n",
            "           3       0.36      0.43      0.39       183\n",
            "           4       0.50      0.42      0.46       205\n",
            "           5       0.62      0.55      0.58       215\n",
            "           6       0.51      0.51      0.51       193\n",
            "           7       0.28      0.55      0.37       196\n",
            "           8       0.50      0.49      0.50       168\n",
            "           9       0.58      0.52      0.55       211\n",
            "          10       0.62      0.60      0.61       198\n",
            "          11       0.67      0.58      0.62       201\n",
            "          12       0.34      0.32      0.33       202\n",
            "          13       0.60      0.57      0.58       194\n",
            "          14       0.53      0.52      0.53       189\n",
            "          15       0.47      0.44      0.45       202\n",
            "          16       0.45      0.46      0.46       188\n",
            "          17       0.65      0.57      0.61       182\n",
            "          18       0.28      0.25      0.26       159\n",
            "          19       0.16      0.14      0.15       136\n",
            "\n",
            "    accuracy                           0.46      3770\n",
            "   macro avg       0.46      0.45      0.46      3770\n",
            "weighted avg       0.47      0.46      0.46      3770\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Naive Bayes model\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "nb_predictions = nb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Naive Bayes Classifier Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, nb_predictions)}\")\n",
        "print(classification_report(y_test, nb_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANCUJK8OEKpb",
        "outputId": "58056ff5-1615-4bc2-c3b8-40e270af3a81"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Classifier Performance:\n",
            "Accuracy: 0.7196286472148541\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.27      0.40       151\n",
            "           1       0.70      0.68      0.69       202\n",
            "           2       0.67      0.66      0.66       195\n",
            "           3       0.55      0.78      0.64       183\n",
            "           4       0.87      0.67      0.75       205\n",
            "           5       0.90      0.81      0.85       215\n",
            "           6       0.79      0.70      0.74       193\n",
            "           7       0.84      0.76      0.79       196\n",
            "           8       0.49      0.77      0.60       168\n",
            "           9       0.92      0.83      0.88       211\n",
            "          10       0.88      0.92      0.90       198\n",
            "          11       0.70      0.85      0.77       201\n",
            "          12       0.85      0.62      0.72       202\n",
            "          13       0.91      0.86      0.88       194\n",
            "          14       0.80      0.83      0.81       189\n",
            "          15       0.42      0.94      0.58       202\n",
            "          16       0.70      0.79      0.74       188\n",
            "          17       0.78      0.83      0.80       182\n",
            "          18       0.92      0.43      0.58       159\n",
            "          19       0.80      0.03      0.06       136\n",
            "\n",
            "    accuracy                           0.72      3770\n",
            "   macro avg       0.76      0.70      0.69      3770\n",
            "weighted avg       0.76      0.72      0.71      3770\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "The outputs display the accuracy and classification reports for both models. The classification report includes precision, recall, f1-score, and support for each of the 20 categories.\n",
        "Decision tree classifier may achieve decent accuracy, but could struggle with overfitting. Naive Bayes classifier is likely to perform faster and have consistent performance but may struggle with topics that require capturing feature dependancies.\n",
        "Therefore, model selection will rely on various factors. For a more interpretable and complex model, decision tree would fit best whereas Naive Bayes would be optimal for a fast, scalable and simple solution."
      ],
      "metadata": {
        "id": "4F8P0fKiG-AM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Improving the accuracy of the classifiers\n",
        "# Using the Random forest classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "rf_predictions = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Random Forest Classifier Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, rf_predictions)}\")\n",
        "print(classification_report(y_test, rf_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ss9ZgDjMae_",
        "outputId": "eb09bf9a-0d51-4b20-950c-acd0f312ad50"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier Performance:\n",
            "Accuracy: 0.6692307692307692\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.43      0.48       151\n",
            "           1       0.61      0.58      0.60       202\n",
            "           2       0.60      0.67      0.63       195\n",
            "           3       0.54      0.67      0.60       183\n",
            "           4       0.80      0.67      0.73       205\n",
            "           5       0.79      0.76      0.77       215\n",
            "           6       0.69      0.68      0.68       193\n",
            "           7       0.44      0.75      0.56       196\n",
            "           8       0.61      0.65      0.63       168\n",
            "           9       0.79      0.76      0.77       211\n",
            "          10       0.81      0.86      0.83       198\n",
            "          11       0.83      0.77      0.80       201\n",
            "          12       0.59      0.54      0.57       202\n",
            "          13       0.79      0.74      0.77       194\n",
            "          14       0.77      0.75      0.76       189\n",
            "          15       0.62      0.78      0.69       202\n",
            "          16       0.65      0.72      0.68       188\n",
            "          17       0.78      0.75      0.77       182\n",
            "          18       0.66      0.42      0.51       159\n",
            "          19       0.52      0.17      0.26       136\n",
            "\n",
            "    accuracy                           0.67      3770\n",
            "   macro avg       0.67      0.66      0.65      3770\n",
            "weighted avg       0.68      0.67      0.67      3770\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Random Forest classifier improves the accuracy compared to a single Decision Tree because it reduces overfitting and provides better generalization.\n",
        "Fine-tuning the hyperparameters further can help optimize performance."
      ],
      "metadata": {
        "id": "AOZmd_jVOWrZ"
      }
    }
  ]
}