{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muthon1/DataScience/blob/main/Phase_3_Hyper_Parameter_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_mqWR_YT5RN",
        "outputId": "01635216-f6f3-4819-9c36-5956c126a7b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install numpy pandas scikit-learn matplotlib\n",
        "\n",
        "# Importing the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkypurj3UGdy"
      },
      "outputs": [],
      "source": [
        "# Load the 20 Newsgroup dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Tuning\n",
        "# Vectorize text using TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(newsgroups.data)\n",
        "y = newsgroups.target\n",
        "\n",
        "# Split data into training and test sets\n",
        "# Use the vectorized data 'X' instead of 'newsgroups.data'\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV  # Import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np  # Import numpy for random distributions\n",
        "\n",
        "# Define the parameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'criterion': ['entropy'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
        "}\n",
        "\n",
        "# Create the Decision Tree classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Set up RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(dt_classifier, param_dist, n_iter=20, cv=5, n_jobs=-1, verbose=2, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Output the best parameters and score\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "print(\"Best cross-validation score: \", random_search.best_score_)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_dt_classifier = random_search.best_estimator_\n",
        "dt_predictions = best_dt_classifier.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZSPQLrwm3xL",
        "outputId": "f883d858-23e3-4278-c961-bc7dc7d4b3f1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "25 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "18 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "7 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.2587569         nan 0.12562981 0.09292905        nan\n",
            " 0.14035556 0.06633072 0.06593274 0.25013408        nan 0.29311423\n",
            " 0.12483396 0.14035556 0.07488717 0.0657338  0.21272237 0.06566746\n",
            "        nan 0.29596656]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 20, 'criterion': 'entropy'}\n",
            "Best cross-validation score:  0.295966564172946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes Hyperparameter Tuning\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Define the parameter grid for tuning Naive Bayes\n",
        "param_grid = {\n",
        "    'alpha': [0.1, 0.5, 1.0, 1.5, 2.0],   # Smoothing parameter (Laplace smoothing)\n",
        "    'fit_prior': [True, False],            # Whether to learn class prior probabilities or not\n",
        "}\n",
        "\n",
        "# Create the Multinomial Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Set up GridSearchCV to find the best parameters using cross-validation\n",
        "grid_search = GridSearchCV(nb_classifier, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Output the best parameters and best cross-validation score\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best cross-validation score: \", grid_search.best_score_)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_nb_classifier = grid_search.best_estimator_\n",
        "nb_predictions = best_nb_classifier.predict(X_test)\n",
        "\n",
        "print(\"Naive Bayes (Tuned) Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, nb_predictions)}\")\n",
        "print(classification_report(y_test, nb_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6sM_Q4yl8j1",
        "outputId": "bd30ce6d-77b6-43a9-e5c8-cc4892f50bd7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best parameters found:  {'alpha': 0.1, 'fit_prior': False}\n",
            "Best cross-validation score:  0.7565004112945439\n",
            "Naive Bayes (Tuned) Performance:\n",
            "Accuracy: 0.7546419098143236\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.52      0.42       151\n",
            "           1       0.73      0.75      0.74       202\n",
            "           2       0.71      0.65      0.68       195\n",
            "           3       0.61      0.77      0.68       183\n",
            "           4       0.83      0.73      0.78       205\n",
            "           5       0.89      0.82      0.86       215\n",
            "           6       0.84      0.70      0.76       193\n",
            "           7       0.83      0.80      0.81       196\n",
            "           8       0.84      0.76      0.80       168\n",
            "           9       0.93      0.85      0.89       211\n",
            "          10       0.93      0.90      0.92       198\n",
            "          11       0.79      0.82      0.80       201\n",
            "          12       0.81      0.70      0.75       202\n",
            "          13       0.88      0.87      0.88       194\n",
            "          14       0.82      0.85      0.83       189\n",
            "          15       0.58      0.92      0.71       202\n",
            "          16       0.68      0.81      0.74       188\n",
            "          17       0.79      0.85      0.81       182\n",
            "          18       0.75      0.61      0.67       159\n",
            "          19       0.82      0.17      0.28       136\n",
            "\n",
            "    accuracy                           0.75      3770\n",
            "   macro avg       0.77      0.74      0.74      3770\n",
            "weighted avg       0.78      0.75      0.75      3770\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "import numpy as np\n",
        "\n",
        "#Feature Selection (if not already done)\n",
        "selector = SelectKBest(chi2, k=1000)  # Choose desired k value\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Define a Reduced Parameter Distribution\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 150],  # Reduced range\n",
        "    'criterion': ['entropy'],\n",
        "    'max_depth': [None, 10, 20],  # Fewer options\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2'],  # Reduced options\n",
        "}\n",
        "\n",
        "# Create the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Set up RandomizedSearchCV with Reduced Iterations and Folds\n",
        "random_search = RandomizedSearchCV(\n",
        "    rf_classifier,\n",
        "    param_dist,\n",
        "    n_iter=10,  # Reduced iterations\n",
        "    cv=3,       # Reduced folds (if necessary)\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "# Using a subset:\n",
        "# X_subset = X_train_selected[:5000]\n",
        "# y_subset = y_train[:5000]\n",
        "# random_search.fit(X_subset, y_subset)\n",
        "\n",
        "# Fit on the full (selected) data:\n",
        "random_search.fit(X_train_selected, y_train)\n",
        "\n",
        "# Output the best parameters and score\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "print(\"Best cross-validation score: \", random_search.best_score_)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_rf_classifier = random_search.best_estimator_\n",
        "rf_predictions = best_rf_classifier.predict(X_test_selected)\n",
        "\n",
        "print(\"Random Forest (Tuned) Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, rf_predictions)}\")\n",
        "print(classification_report(y_test, rf_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9pSFZMe-tjS",
        "outputId": "19e88745-57bf-4b78-9b59-dda63987053b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Best parameters found:  {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None, 'criterion': 'entropy'}\n",
            "Best cross-validation score:  0.5848370298659243\n",
            "Random Forest (Tuned) Performance:\n",
            "Accuracy: 0.5822281167108754\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.32      0.40       151\n",
            "           1       0.58      0.48      0.53       202\n",
            "           2       0.57      0.64      0.60       195\n",
            "           3       0.44      0.58      0.50       183\n",
            "           4       0.69      0.54      0.61       205\n",
            "           5       0.76      0.64      0.69       215\n",
            "           6       0.64      0.64      0.64       193\n",
            "           7       0.69      0.62      0.65       196\n",
            "           8       0.24      0.72      0.36       168\n",
            "           9       0.74      0.57      0.64       211\n",
            "          10       0.70      0.83      0.76       198\n",
            "          11       0.81      0.63      0.71       201\n",
            "          12       0.45      0.32      0.37       202\n",
            "          13       0.72      0.69      0.70       194\n",
            "          14       0.72      0.65      0.68       189\n",
            "          15       0.57      0.78      0.66       202\n",
            "          16       0.61      0.64      0.62       188\n",
            "          17       0.80      0.73      0.76       182\n",
            "          18       0.44      0.35      0.39       159\n",
            "          19       0.60      0.07      0.12       136\n",
            "\n",
            "    accuracy                           0.58      3770\n",
            "   macro avg       0.62      0.57      0.57      3770\n",
            "weighted avg       0.62      0.58      0.58      3770\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5EJHJYMre92"
      },
      "source": [
        "# Significance of Hyperparameter tuning\n",
        "Hyperparameter tuning is crucial for building high-performance machine learning models. Selection of the right hyperparameters significantly improves accuracy, reduces overfitting and speeds up training making the models more efficient."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQjuThiJHTt7G93yO8hDXA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}